{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Training/image/SN10_Forest_IMAGE Done./\n",
      "Dataset/Training/image/SN10_Forest_SH Done./\n",
      "Dataset/Training/label/SN10_Forest_Carbon Done./\n",
      "Dataset/Training/label/SN10_Forest_GT Done./\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)), transforms\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[0;32m     15\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(CarbonDataset(path,transform, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m x, carbon,gt \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m gt_pred, carbon_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     19\u001b[0m corr \u001b[38;5;241m=\u001b[39m calculate_correlation(gt_pred, gt)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dust\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:152\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.dpt import DPTSegmentationWithCarbon\n",
    "from dataset import CarbonDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from models.util import calculate_correlation\n",
    "kwargs = {\n",
    "    \"num_classes\": 7,\n",
    "}\n",
    "\n",
    "model = DPTSegmentationWithCarbon(num_classes=kwargs[\"num_classes\"])\n",
    "path = \"Dataset/Training/image/SN10_Forest_IMAGE\"\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "data_loader = DataLoader(CarbonDataset(path,transform, mode = \"Train\"), batch_size=2, shuffle=True)\n",
    "\n",
    "x, carbon,gt = data_loader.__iter__().__next__()\n",
    "gt_pred, carbon_pred = model(x)\n",
    "corr = calculate_correlation(gt_pred, gt)\n",
    "print(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "  (model): VisionTransformer(\n",
      "    (patch_embed): HybridEmbed(\n",
      "      (backbone): ResNetV2(\n",
      "        (stem): Sequential(\n",
      "          (conv): StdConv2dSame(4, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
      "          (norm): GroupNormAct(\n",
      "            32, 64, eps=1e-05, affine=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU(inplace=True)\n",
      "          )\n",
      "          (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
      "        )\n",
      "        (stages): Sequential(\n",
      "          (0): ResNetStage(\n",
      "            (blocks): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (downsample): DownsampleConv(\n",
      "                  (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (norm): GroupNormAct(\n",
      "                    32, 256, eps=1e-05, affine=True\n",
      "                    (drop): Identity()\n",
      "                    (act): Identity()\n",
      "                  )\n",
      "                )\n",
      "                (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 64, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 64, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 64, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 64, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (2): Bottleneck(\n",
      "                (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 64, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 64, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): ResNetStage(\n",
      "            (blocks): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (downsample): DownsampleConv(\n",
      "                  (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                  (norm): GroupNormAct(\n",
      "                    32, 512, eps=1e-05, affine=True\n",
      "                    (drop): Identity()\n",
      "                    (act): Identity()\n",
      "                  )\n",
      "                )\n",
      "                (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 512, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 512, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (2): Bottleneck(\n",
      "                (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 512, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (3): Bottleneck(\n",
      "                (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 128, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 512, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): ResNetStage(\n",
      "            (blocks): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (downsample): DownsampleConv(\n",
      "                  (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                  (norm): GroupNormAct(\n",
      "                    32, 1024, eps=1e-05, affine=True\n",
      "                    (drop): Identity()\n",
      "                    (act): Identity()\n",
      "                  )\n",
      "                )\n",
      "                (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (2): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (3): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (4): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (5): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (6): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (7): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (8): Bottleneck(\n",
      "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm1): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm2): GroupNormAct(\n",
      "                  32, 256, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): ReLU(inplace=True)\n",
      "                )\n",
      "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm3): GroupNormAct(\n",
      "                  32, 1024, eps=1e-05, affine=True\n",
      "                  (drop): Identity()\n",
      "                  (act): Identity()\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): Identity()\n",
      "        (head): ClassifierHead(\n",
      "          (global_pool): SelectAdaptivePool2d(pool_type=, flatten=Identity())\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "          (fc): Identity()\n",
      "          (flatten): Identity()\n",
      "        )\n",
      "      )\n",
      "      (proj): Conv2d(1024, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (patch_drop): Identity()\n",
      "    (norm_pre): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (fc_norm): Identity()\n",
      "    (head_drop): Dropout(p=0.0, inplace=False)\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      "  (act_postprocess1): Sequential(\n",
      "    (0): Identity()\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (act_postprocess2): Sequential(\n",
      "    (0): Identity()\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (act_postprocess3): Sequential(\n",
      "    (0): ProjectReadout(\n",
      "      (project): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=768, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "    (1): Transpose()\n",
      "    (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "    (3): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (act_postprocess4): Sequential(\n",
      "    (0): ProjectReadout(\n",
      "      (project): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=768, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "    (1): Transpose()\n",
      "    (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
      "    (3): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "  (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer3_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (refinenet1): FeatureFusionBlock_custom(\n",
      "    (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (resConfUnit1): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (resConfUnit2): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (skip_add): FloatFunctional(\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "  )\n",
      "  (refinenet2): FeatureFusionBlock_custom(\n",
      "    (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (resConfUnit1): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (resConfUnit2): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (skip_add): FloatFunctional(\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "  )\n",
      "  (refinenet3): FeatureFusionBlock_custom(\n",
      "    (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (resConfUnit1): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (resConfUnit2): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (skip_add): FloatFunctional(\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "  )\n",
      "  (refinenet4): FeatureFusionBlock_custom(\n",
      "    (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (resConfUnit1): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (resConfUnit2): ResidualConvUnit_custom(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (skip_add): FloatFunctional(\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "  )\n",
      "  (output_conv): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(256, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Interpolate()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                           Input Shape               Output Shape              Param #\n",
       "======================================================================================================================================================\n",
       "Segformer (Segformer)                                                       [2, 4, 224, 224]          [2, 4, 56, 56]            --\n",
       "├─MiT (mit): 1-1                                                            [2, 4, 224, 224]          [2, 32, 56, 56]           --\n",
       "│    └─ModuleList (stages): 2-1                                             --                        --                        --\n",
       "│    │    └─ModuleList (0): 3-1                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-1                                            [2, 4, 224, 224]          [2, 196, 3136]            --\n",
       "│    │    │    └─Conv2d (1): 4-2                                            [2, 196, 56, 56]          [2, 32, 56, 56]           6,304\n",
       "│    │    │    └─ModuleList (2): 4-3                                        --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-1                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-1                                 [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-1                       [2, 32, 56, 56]           [2, 32, 56, 56]           64\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-2            [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-1                     [2, 32, 56, 56]           [2, 32, 56, 56]           1,024\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-2                    [2, 32, 56, 56]           [2, 64, 7, 7]             131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-3                   [2, 32, 56, 56]           [2, 32, 56, 56]           1,024\n",
       "│    │    │    │    │    └─PreNorm (1): 6-2                                 [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-3                       [2, 32, 56, 56]           [2, 32, 56, 56]           64\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-4                    [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-4                  [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-1                   [2, 32, 56, 56]           [2, 256, 56, 56]          8,448\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-2                 [2, 256, 56, 56]          [2, 256, 56, 56]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-1       [2, 256, 56, 56]          [2, 256, 56, 56]          68,352\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-3                     [2, 256, 56, 56]          [2, 256, 56, 56]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-4                   [2, 256, 56, 56]          [2, 32, 56, 56]           8,224\n",
       "│    │    │    │    └─ModuleList (1): 5-2                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-3                                 [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-5                       [2, 32, 56, 56]           [2, 32, 56, 56]           64\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-6            [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-5                     [2, 32, 56, 56]           [2, 32, 56, 56]           1,024\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-6                    [2, 32, 56, 56]           [2, 64, 7, 7]             131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-7                   [2, 32, 56, 56]           [2, 32, 56, 56]           1,024\n",
       "│    │    │    │    │    └─PreNorm (1): 6-4                                 [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-7                       [2, 32, 56, 56]           [2, 32, 56, 56]           64\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-8                    [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-8                  [2, 32, 56, 56]           [2, 32, 56, 56]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-5                   [2, 32, 56, 56]           [2, 256, 56, 56]          8,448\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-6                 [2, 256, 56, 56]          [2, 256, 56, 56]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-2       [2, 256, 56, 56]          [2, 256, 56, 56]          68,352\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-7                     [2, 256, 56, 56]          [2, 256, 56, 56]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-8                   [2, 256, 56, 56]          [2, 32, 56, 56]           8,224\n",
       "│    │    └─ModuleList (1): 3-2                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-4                                            [2, 32, 56, 56]           [2, 288, 784]             --\n",
       "│    │    │    └─Conv2d (1): 4-5                                            [2, 288, 28, 28]          [2, 64, 28, 28]           18,496\n",
       "│    │    │    └─ModuleList (2): 4-6                                        --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-3                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-5                                 [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-9                       [2, 64, 28, 28]           [2, 64, 28, 28]           128\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-10           [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-9                     [2, 64, 28, 28]           [2, 64, 28, 28]           4,096\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-10                   [2, 64, 28, 28]           [2, 128, 7, 7]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-11                  [2, 64, 28, 28]           [2, 64, 28, 28]           4,096\n",
       "│    │    │    │    │    └─PreNorm (1): 6-6                                 [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-11                      [2, 64, 28, 28]           [2, 64, 28, 28]           128\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-12                   [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-12                 [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-9                   [2, 64, 28, 28]           [2, 512, 28, 28]          33,280\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-10                [2, 512, 28, 28]          [2, 512, 28, 28]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-3       [2, 512, 28, 28]          [2, 512, 28, 28]          267,776\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-11                    [2, 512, 28, 28]          [2, 512, 28, 28]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-12                  [2, 512, 28, 28]          [2, 64, 28, 28]           32,832\n",
       "│    │    │    │    └─ModuleList (1): 5-4                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-7                                 [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-13                      [2, 64, 28, 28]           [2, 64, 28, 28]           128\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-14           [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-13                    [2, 64, 28, 28]           [2, 64, 28, 28]           4,096\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-14                   [2, 64, 28, 28]           [2, 128, 7, 7]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-15                  [2, 64, 28, 28]           [2, 64, 28, 28]           4,096\n",
       "│    │    │    │    │    └─PreNorm (1): 6-8                                 [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-15                      [2, 64, 28, 28]           [2, 64, 28, 28]           128\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-16                   [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-16                 [2, 64, 28, 28]           [2, 64, 28, 28]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-13                  [2, 64, 28, 28]           [2, 512, 28, 28]          33,280\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-14                [2, 512, 28, 28]          [2, 512, 28, 28]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-4       [2, 512, 28, 28]          [2, 512, 28, 28]          267,776\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-15                    [2, 512, 28, 28]          [2, 512, 28, 28]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-16                  [2, 512, 28, 28]          [2, 64, 28, 28]           32,832\n",
       "│    │    └─ModuleList (2): 3-3                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-7                                            [2, 64, 28, 28]           [2, 576, 196]             --\n",
       "│    │    │    └─Conv2d (1): 4-8                                            [2, 576, 14, 14]          [2, 160, 14, 14]          92,320\n",
       "│    │    │    └─ModuleList (2): 4-9                                        --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-5                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-9                                 [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-17                      [2, 160, 14, 14]          [2, 160, 14, 14]          320\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-18           [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-17                    [2, 160, 14, 14]          [2, 160, 14, 14]          25,600\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-18                   [2, 160, 14, 14]          [2, 320, 7, 7]            204,800\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-19                  [2, 160, 14, 14]          [2, 160, 14, 14]          25,600\n",
       "│    │    │    │    │    └─PreNorm (1): 6-10                                [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-19                      [2, 160, 14, 14]          [2, 160, 14, 14]          320\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-20                   [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-20                 [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-17                  [2, 160, 14, 14]          [2, 640, 14, 14]          103,040\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-18                [2, 640, 14, 14]          [2, 640, 14, 14]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-5       [2, 640, 14, 14]          [2, 640, 14, 14]          416,640\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-19                    [2, 640, 14, 14]          [2, 640, 14, 14]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-20                  [2, 640, 14, 14]          [2, 160, 14, 14]          102,560\n",
       "│    │    │    │    └─ModuleList (1): 5-6                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-11                                [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-21                      [2, 160, 14, 14]          [2, 160, 14, 14]          320\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-22           [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-21                    [2, 160, 14, 14]          [2, 160, 14, 14]          25,600\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-22                   [2, 160, 14, 14]          [2, 320, 7, 7]            204,800\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-23                  [2, 160, 14, 14]          [2, 160, 14, 14]          25,600\n",
       "│    │    │    │    │    └─PreNorm (1): 6-12                                [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-23                      [2, 160, 14, 14]          [2, 160, 14, 14]          320\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-24                   [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-24                 [2, 160, 14, 14]          [2, 160, 14, 14]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-21                  [2, 160, 14, 14]          [2, 640, 14, 14]          103,040\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-22                [2, 640, 14, 14]          [2, 640, 14, 14]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-6       [2, 640, 14, 14]          [2, 640, 14, 14]          416,640\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-23                    [2, 640, 14, 14]          [2, 640, 14, 14]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-24                  [2, 640, 14, 14]          [2, 160, 14, 14]          102,560\n",
       "│    │    └─ModuleList (3): 3-4                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-10                                           [2, 160, 14, 14]          [2, 1440, 49]             --\n",
       "│    │    │    └─Conv2d (1): 4-11                                           [2, 1440, 7, 7]           [2, 256, 7, 7]            368,896\n",
       "│    │    │    └─ModuleList (2): 4-12                                       --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-7                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-13                                [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-25                      [2, 256, 7, 7]            [2, 256, 7, 7]            512\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-26           [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-25                    [2, 256, 7, 7]            [2, 256, 7, 7]            65,536\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-26                   [2, 256, 7, 7]            [2, 512, 7, 7]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-27                  [2, 256, 7, 7]            [2, 256, 7, 7]            65,536\n",
       "│    │    │    │    │    └─PreNorm (1): 6-14                                [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-27                      [2, 256, 7, 7]            [2, 256, 7, 7]            512\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-28                   [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-28                 [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-25                  [2, 256, 7, 7]            [2, 1024, 7, 7]           263,168\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-26                [2, 1024, 7, 7]           [2, 1024, 7, 7]           --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-7       [2, 1024, 7, 7]           [2, 1024, 7, 7]           1,059,840\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-27                    [2, 1024, 7, 7]           [2, 1024, 7, 7]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-28                  [2, 1024, 7, 7]           [2, 256, 7, 7]            262,400\n",
       "│    │    │    │    └─ModuleList (1): 5-8                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-15                                [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-29                      [2, 256, 7, 7]            [2, 256, 7, 7]            512\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-30           [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-29                    [2, 256, 7, 7]            [2, 256, 7, 7]            65,536\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-30                   [2, 256, 7, 7]            [2, 512, 7, 7]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-31                  [2, 256, 7, 7]            [2, 256, 7, 7]            65,536\n",
       "│    │    │    │    │    └─PreNorm (1): 6-16                                [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-31                      [2, 256, 7, 7]            [2, 256, 7, 7]            512\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-32                   [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-32                 [2, 256, 7, 7]            [2, 256, 7, 7]            --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-29                  [2, 256, 7, 7]            [2, 1024, 7, 7]           263,168\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-30                [2, 1024, 7, 7]           [2, 1024, 7, 7]           --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-8       [2, 1024, 7, 7]           [2, 1024, 7, 7]           1,059,840\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-31                    [2, 1024, 7, 7]           [2, 1024, 7, 7]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-32                  [2, 1024, 7, 7]           [2, 256, 7, 7]            262,400\n",
       "├─ModuleList (to_fused): 1-2                                                --                        --                        --\n",
       "│    └─Sequential (0): 2-2                                                  [2, 32, 56, 56]           [2, 256, 56, 56]          --\n",
       "│    │    └─Conv2d (0): 3-5                                                 [2, 32, 56, 56]           [2, 256, 56, 56]          8,448\n",
       "│    │    └─Upsample (1): 3-6                                               [2, 256, 56, 56]          [2, 256, 56, 56]          --\n",
       "│    └─Sequential (1): 2-3                                                  [2, 64, 28, 28]           [2, 256, 56, 56]          --\n",
       "│    │    └─Conv2d (0): 3-7                                                 [2, 64, 28, 28]           [2, 256, 28, 28]          16,640\n",
       "│    │    └─Upsample (1): 3-8                                               [2, 256, 28, 28]          [2, 256, 56, 56]          --\n",
       "│    └─Sequential (2): 2-4                                                  [2, 160, 14, 14]          [2, 256, 56, 56]          --\n",
       "│    │    └─Conv2d (0): 3-9                                                 [2, 160, 14, 14]          [2, 256, 14, 14]          41,216\n",
       "│    │    └─Upsample (1): 3-10                                              [2, 256, 14, 14]          [2, 256, 56, 56]          --\n",
       "│    └─Sequential (3): 2-5                                                  [2, 256, 7, 7]            [2, 256, 56, 56]          --\n",
       "│    │    └─Conv2d (0): 3-11                                                [2, 256, 7, 7]            [2, 256, 7, 7]            65,792\n",
       "│    │    └─Upsample (1): 3-12                                              [2, 256, 7, 7]            [2, 256, 56, 56]          --\n",
       "├─Sequential (to_segmentation): 1-3                                         [2, 1024, 56, 56]         [2, 4, 56, 56]            --\n",
       "│    └─Conv2d (0): 2-6                                                      [2, 1024, 56, 56]         [2, 256, 56, 56]          262,400\n",
       "│    └─Conv2d (1): 2-7                                                      [2, 256, 56, 56]          [2, 4, 56, 56]            1,028\n",
       "======================================================================================================================================================\n",
       "Total params: 7,719,812\n",
       "Trainable params: 7,719,812\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.04\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 1.61\n",
       "Forward/backward pass size (MB): 198.40\n",
       "Params size (MB): 30.88\n",
       "Estimated Total Size (MB): 230.88\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# 모델과 입력 데이터의 크기를 summary 함수에 전달\n",
    "summary(model,col_names=(\"input_size\",\"output_size\",\"num_params\",),row_settings=('var_names','depth'), depth=10,input_size=(2, 4, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                           Input Shape               Output Shape              Param #\n",
       "======================================================================================================================================================\n",
       "Segformer (Segformer)                                                       [2, 4, 256, 256]          [2, 4, 64, 64]            --\n",
       "├─MiT (mit): 1-1                                                            [2, 4, 256, 256]          [2, 32, 64, 64]           --\n",
       "│    └─ModuleList (stages): 2-1                                             --                        --                        --\n",
       "│    │    └─ModuleList (0): 3-1                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-1                                            [2, 4, 256, 256]          [2, 196, 4096]            --\n",
       "│    │    │    └─Conv2d (1): 4-2                                            [2, 196, 64, 64]          [2, 32, 64, 64]           6,304\n",
       "│    │    │    └─ModuleList (2): 4-3                                        --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-1                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-1                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-1                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-2            [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-1                     [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-2                    [2, 32, 64, 64]           [2, 64, 8, 8]             131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-3                   [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    │    └─PreNorm (1): 6-2                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-3                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-4                    [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-4                  [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-1                   [2, 32, 64, 64]           [2, 256, 64, 64]          8,448\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-2                 [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-1       [2, 256, 64, 64]          [2, 256, 64, 64]          68,352\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-3                     [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-4                   [2, 256, 64, 64]          [2, 32, 64, 64]           8,224\n",
       "│    │    │    │    └─ModuleList (1): 5-2                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-3                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-5                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-6            [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-5                     [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-6                    [2, 32, 64, 64]           [2, 64, 8, 8]             131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-7                   [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    │    └─PreNorm (1): 6-4                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-7                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-8                    [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-8                  [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-5                   [2, 32, 64, 64]           [2, 256, 64, 64]          8,448\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-6                 [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-2       [2, 256, 64, 64]          [2, 256, 64, 64]          68,352\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-7                     [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-8                   [2, 256, 64, 64]          [2, 32, 64, 64]           8,224\n",
       "│    │    └─ModuleList (1): 3-2                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-4                                            [2, 32, 64, 64]           [2, 288, 1024]            --\n",
       "│    │    │    └─Conv2d (1): 4-5                                            [2, 288, 32, 32]          [2, 64, 32, 32]           18,496\n",
       "│    │    │    └─ModuleList (2): 4-6                                        --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-3                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-5                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-9                       [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-10           [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-9                     [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-10                   [2, 64, 32, 32]           [2, 128, 8, 8]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-11                  [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    │    └─PreNorm (1): 6-6                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-11                      [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-12                   [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-12                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-9                   [2, 64, 32, 32]           [2, 512, 32, 32]          33,280\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-10                [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-3       [2, 512, 32, 32]          [2, 512, 32, 32]          267,776\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-11                    [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-12                  [2, 512, 32, 32]          [2, 64, 32, 32]           32,832\n",
       "│    │    │    │    └─ModuleList (1): 5-4                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-7                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-13                      [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-14           [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-13                    [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-14                   [2, 64, 32, 32]           [2, 128, 8, 8]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-15                  [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    │    └─PreNorm (1): 6-8                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-15                      [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-16                   [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-16                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-13                  [2, 64, 32, 32]           [2, 512, 32, 32]          33,280\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-14                [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-4       [2, 512, 32, 32]          [2, 512, 32, 32]          267,776\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-15                    [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-16                  [2, 512, 32, 32]          [2, 64, 32, 32]           32,832\n",
       "│    │    └─ModuleList (2): 3-3                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-7                                            [2, 64, 32, 32]           [2, 576, 256]             --\n",
       "│    │    │    └─Conv2d (1): 4-8                                            [2, 576, 16, 16]          [2, 160, 16, 16]          92,320\n",
       "│    │    │    └─ModuleList (2): 4-9                                        --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-5                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-9                                 [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-17                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-18           [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-17                    [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-18                   [2, 160, 16, 16]          [2, 320, 8, 8]            204,800\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-19                  [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    │    └─PreNorm (1): 6-10                                [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-19                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-20                   [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-20                 [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-17                  [2, 160, 16, 16]          [2, 640, 16, 16]          103,040\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-18                [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-5       [2, 640, 16, 16]          [2, 640, 16, 16]          416,640\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-19                    [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-20                  [2, 640, 16, 16]          [2, 160, 16, 16]          102,560\n",
       "│    │    │    │    └─ModuleList (1): 5-6                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-11                                [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-21                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-22           [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-21                    [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-22                   [2, 160, 16, 16]          [2, 320, 8, 8]            204,800\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-23                  [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    │    └─PreNorm (1): 6-12                                [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-23                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-24                   [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-24                 [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-21                  [2, 160, 16, 16]          [2, 640, 16, 16]          103,040\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-22                [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-6       [2, 640, 16, 16]          [2, 640, 16, 16]          416,640\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-23                    [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-24                  [2, 640, 16, 16]          [2, 160, 16, 16]          102,560\n",
       "│    │    └─ModuleList (3): 3-4                                             --                        --                        --\n",
       "│    │    │    └─Unfold (0): 4-10                                           [2, 160, 16, 16]          [2, 1440, 64]             --\n",
       "│    │    │    └─Conv2d (1): 4-11                                           [2, 1440, 8, 8]           [2, 256, 8, 8]            368,896\n",
       "│    │    │    └─ModuleList (2): 4-12                                       --                        --                        --\n",
       "│    │    │    │    └─ModuleList (0): 5-7                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-13                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-25                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-26           [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-25                    [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-26                   [2, 256, 8, 8]            [2, 512, 8, 8]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-27                  [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    │    └─PreNorm (1): 6-14                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-27                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-28                   [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-28                 [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-25                  [2, 256, 8, 8]            [2, 1024, 8, 8]           263,168\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-26                [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-7       [2, 1024, 8, 8]           [2, 1024, 8, 8]           1,059,840\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-27                    [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-28                  [2, 1024, 8, 8]           [2, 256, 8, 8]            262,400\n",
       "│    │    │    │    └─ModuleList (1): 5-8                                   --                        --                        --\n",
       "│    │    │    │    │    └─PreNorm (0): 6-15                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-29                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    │    └─EfficientSelfAttention (fn): 7-30           [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_q): 8-29                    [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_kv): 8-30                   [2, 256, 8, 8]            [2, 512, 8, 8]            131,072\n",
       "│    │    │    │    │    │    │    └─Conv2d (to_out): 8-31                  [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    │    └─PreNorm (1): 6-16                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─LayerNorm (norm): 7-31                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    │    └─MixFeedForward (fn): 7-32                   [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    └─Sequential (net): 8-32                 [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (0): 9-29                  [2, 256, 8, 8]            [2, 1024, 8, 8]           263,168\n",
       "│    │    │    │    │    │    │    │    └─DsConv2d (1): 9-30                [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    │    └─Sequential (net): 10-8       [2, 1024, 8, 8]           [2, 1024, 8, 8]           1,059,840\n",
       "│    │    │    │    │    │    │    │    └─GELU (2): 9-31                    [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    └─Conv2d (3): 9-32                  [2, 1024, 8, 8]           [2, 256, 8, 8]            262,400\n",
       "├─ModuleList (to_fused): 1-2                                                --                        --                        --\n",
       "│    └─Sequential (0): 2-2                                                  [2, 32, 64, 64]           [2, 256, 64, 64]          --\n",
       "│    │    └─Conv2d (0): 3-5                                                 [2, 32, 64, 64]           [2, 256, 64, 64]          8,448\n",
       "│    │    └─Upsample (1): 3-6                                               [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    └─Sequential (1): 2-3                                                  [2, 64, 32, 32]           [2, 256, 64, 64]          --\n",
       "│    │    └─Conv2d (0): 3-7                                                 [2, 64, 32, 32]           [2, 256, 32, 32]          16,640\n",
       "│    │    └─Upsample (1): 3-8                                               [2, 256, 32, 32]          [2, 256, 64, 64]          --\n",
       "│    └─Sequential (2): 2-4                                                  [2, 160, 16, 16]          [2, 256, 64, 64]          --\n",
       "│    │    └─Conv2d (0): 3-9                                                 [2, 160, 16, 16]          [2, 256, 16, 16]          41,216\n",
       "│    │    └─Upsample (1): 3-10                                              [2, 256, 16, 16]          [2, 256, 64, 64]          --\n",
       "│    └─Sequential (3): 2-5                                                  [2, 256, 8, 8]            [2, 256, 64, 64]          --\n",
       "│    │    └─Conv2d (0): 3-11                                                [2, 256, 8, 8]            [2, 256, 8, 8]            65,792\n",
       "│    │    └─Upsample (1): 3-12                                              [2, 256, 8, 8]            [2, 256, 64, 64]          --\n",
       "├─Sequential (to_segmentation): 1-3                                         [2, 1024, 64, 64]         [2, 4, 64, 64]            --\n",
       "│    └─Conv2d (0): 2-6                                                      [2, 1024, 64, 64]         [2, 256, 64, 64]          262,400\n",
       "│    └─Conv2d (1): 2-7                                                      [2, 256, 64, 64]          [2, 4, 64, 64]            1,028\n",
       "======================================================================================================================================================\n",
       "Total params: 7,719,812\n",
       "Trainable params: 7,719,812\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.58\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 2.10\n",
       "Forward/backward pass size (MB): 259.13\n",
       "Params size (MB): 30.88\n",
       "Estimated Total Size (MB): 292.11\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.segformer_simple import Segformer\n",
    "args = {\n",
    "    'dims': (32, 64, 160, 256),\n",
    "    'heads': (1, 2, 5, 8),\n",
    "    'ff_expansion': (8, 8, 4, 4),\n",
    "    'reduction_ratio': (8, 4, 2, 1),\n",
    "    'num_layers': 2,\n",
    "    'channels': 4,\n",
    "    'decoder_dim': 256,\n",
    "    'num_classes': 4\n",
    "}\n",
    "model = Segformer(**args)\n",
    "summary(model,col_names=(\"input_size\",\"output_size\",\"num_params\",),row_settings=('var_names','depth'), depth=10,input_size=(2, 4, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "fp = 16\n",
    "model_name = \"Segformer\"\n",
    "cls_lambda = 1\n",
    "reg_lambda = 1\n",
    "checkpoint_path = \"checkpoint\"\n",
    "args = {\n",
    "    'num_classes': 4,\n",
    "    'dims': (32, 64, 160, 256),\n",
    "}\n",
    "config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"fp\": fp,\n",
    "    \"model_name\": model_name,\n",
    "    \"cls_lambda\": cls_lambda,\n",
    "    \"reg_lambda\": reg_lambda,\n",
    "    \"checkpoint_path\": checkpoint_path\n",
    "    }.update(args)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.75\n",
      "256.0\n",
      "128.5\n"
     ]
    }
   ],
   "source": [
    "def conv_output(k, s, p, W) :\n",
    "    return ( W - k + 2*p) / s + 1\n",
    "\n",
    "print(conv_output(7, 4, 3, 256))\n",
    "print(conv_output(5, 1, 2, 256))\n",
    "print(conv_output(3, 2, 1, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                      Input Shape               Output Shape              Param #\n",
       "=================================================================================================================================================\n",
       "MiT (MiT)                                                              [2, 4, 256, 256]          [2, 256, 8, 8]            --\n",
       "├─ModuleList (stages): 1-1                                             --                        --                        --\n",
       "│    └─ModuleList (0): 2-1                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-1                                            [2, 4, 256, 256]          [2, 196, 4096]            --\n",
       "│    │    └─Conv2d (1): 3-2                                            [2, 196, 64, 64]          [2, 32, 64, 64]           6,304\n",
       "│    │    └─ModuleList (2): 3-3                                        --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-1                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-1                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-1                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-2            [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-1                     [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-2                    [2, 32, 64, 64]           [2, 64, 8, 8]             131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-3                   [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    └─PreNorm (1): 5-2                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-3                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-4                    [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-4                  [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-1                   [2, 32, 64, 64]           [2, 256, 64, 64]          8,448\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-2                 [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-1        [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-1        [2, 256, 64, 64]          [2, 256, 64, 64]          2,560\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-2        [2, 256, 64, 64]          [2, 256, 64, 64]          65,792\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-3                     [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-4                   [2, 256, 64, 64]          [2, 32, 64, 64]           8,224\n",
       "│    │    │    └─ModuleList (1): 4-2                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-3                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-5                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-6            [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-5                     [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-6                    [2, 32, 64, 64]           [2, 64, 8, 8]             131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-7                   [2, 32, 64, 64]           [2, 32, 64, 64]           1,024\n",
       "│    │    │    │    └─PreNorm (1): 5-4                                 [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-7                       [2, 32, 64, 64]           [2, 32, 64, 64]           64\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-8                    [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-8                  [2, 32, 64, 64]           [2, 32, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-5                   [2, 32, 64, 64]           [2, 256, 64, 64]          8,448\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-6                 [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-2        [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-3        [2, 256, 64, 64]          [2, 256, 64, 64]          2,560\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-4        [2, 256, 64, 64]          [2, 256, 64, 64]          65,792\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-7                     [2, 256, 64, 64]          [2, 256, 64, 64]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-8                   [2, 256, 64, 64]          [2, 32, 64, 64]           8,224\n",
       "│    └─ModuleList (1): 2-2                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-4                                            [2, 32, 64, 64]           [2, 288, 1024]            --\n",
       "│    │    └─Conv2d (1): 3-5                                            [2, 288, 32, 32]          [2, 64, 32, 32]           18,496\n",
       "│    │    └─ModuleList (2): 3-6                                        --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-3                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-5                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-9                       [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-10           [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-9                     [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-10                   [2, 64, 32, 32]           [2, 128, 8, 8]            131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-11                  [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    └─PreNorm (1): 5-6                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-11                      [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-12                   [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-12                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-9                   [2, 64, 32, 32]           [2, 512, 32, 32]          33,280\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-10                [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-3        [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-5        [2, 512, 32, 32]          [2, 512, 32, 32]          5,120\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-6        [2, 512, 32, 32]          [2, 512, 32, 32]          262,656\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-11                    [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-12                  [2, 512, 32, 32]          [2, 64, 32, 32]           32,832\n",
       "│    │    │    └─ModuleList (1): 4-4                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-7                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-13                      [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-14           [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-13                    [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-14                   [2, 64, 32, 32]           [2, 128, 8, 8]            131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-15                  [2, 64, 32, 32]           [2, 64, 32, 32]           4,096\n",
       "│    │    │    │    └─PreNorm (1): 5-8                                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-15                      [2, 64, 32, 32]           [2, 64, 32, 32]           128\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-16                   [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-16                 [2, 64, 32, 32]           [2, 64, 32, 32]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-13                  [2, 64, 32, 32]           [2, 512, 32, 32]          33,280\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-14                [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-4        [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-7        [2, 512, 32, 32]          [2, 512, 32, 32]          5,120\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-8        [2, 512, 32, 32]          [2, 512, 32, 32]          262,656\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-15                    [2, 512, 32, 32]          [2, 512, 32, 32]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-16                  [2, 512, 32, 32]          [2, 64, 32, 32]           32,832\n",
       "│    └─ModuleList (2): 2-3                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-7                                            [2, 64, 32, 32]           [2, 576, 256]             --\n",
       "│    │    └─Conv2d (1): 3-8                                            [2, 576, 16, 16]          [2, 160, 16, 16]          92,320\n",
       "│    │    └─ModuleList (2): 3-9                                        --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-5                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-9                                 [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-17                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-18           [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-17                    [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-18                   [2, 160, 16, 16]          [2, 320, 8, 8]            204,800\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-19                  [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    └─PreNorm (1): 5-10                                [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-19                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-20                   [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-20                 [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-17                  [2, 160, 16, 16]          [2, 640, 16, 16]          103,040\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-18                [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-5        [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-9        [2, 640, 16, 16]          [2, 640, 16, 16]          6,400\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-10       [2, 640, 16, 16]          [2, 640, 16, 16]          410,240\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-19                    [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-20                  [2, 640, 16, 16]          [2, 160, 16, 16]          102,560\n",
       "│    │    │    └─ModuleList (1): 4-6                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-11                                [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-21                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-22           [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-21                    [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-22                   [2, 160, 16, 16]          [2, 320, 8, 8]            204,800\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-23                  [2, 160, 16, 16]          [2, 160, 16, 16]          25,600\n",
       "│    │    │    │    └─PreNorm (1): 5-12                                [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-23                      [2, 160, 16, 16]          [2, 160, 16, 16]          320\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-24                   [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-24                 [2, 160, 16, 16]          [2, 160, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-21                  [2, 160, 16, 16]          [2, 640, 16, 16]          103,040\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-22                [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-6        [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-11       [2, 640, 16, 16]          [2, 640, 16, 16]          6,400\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-12       [2, 640, 16, 16]          [2, 640, 16, 16]          410,240\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-23                    [2, 640, 16, 16]          [2, 640, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-24                  [2, 640, 16, 16]          [2, 160, 16, 16]          102,560\n",
       "│    └─ModuleList (3): 2-4                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-10                                           [2, 160, 16, 16]          [2, 1440, 64]             --\n",
       "│    │    └─Conv2d (1): 3-11                                           [2, 1440, 8, 8]           [2, 256, 8, 8]            368,896\n",
       "│    │    └─ModuleList (2): 3-12                                       --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-7                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-13                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-25                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-26           [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-25                    [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-26                   [2, 256, 8, 8]            [2, 512, 8, 8]            131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-27                  [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    └─PreNorm (1): 5-14                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-27                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-28                   [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-28                 [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-25                  [2, 256, 8, 8]            [2, 1024, 8, 8]           263,168\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-26                [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-7        [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-13       [2, 1024, 8, 8]           [2, 1024, 8, 8]           10,240\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-14       [2, 1024, 8, 8]           [2, 1024, 8, 8]           1,049,600\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-27                    [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-28                  [2, 1024, 8, 8]           [2, 256, 8, 8]            262,400\n",
       "│    │    │    └─ModuleList (1): 4-8                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-15                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-29                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-30           [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-29                    [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-30                   [2, 256, 8, 8]            [2, 512, 8, 8]            131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-31                  [2, 256, 8, 8]            [2, 256, 8, 8]            65,536\n",
       "│    │    │    │    └─PreNorm (1): 5-16                                [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-31                      [2, 256, 8, 8]            [2, 256, 8, 8]            512\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-32                   [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-32                 [2, 256, 8, 8]            [2, 256, 8, 8]            --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-29                  [2, 256, 8, 8]            [2, 1024, 8, 8]           263,168\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-30                [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-8        [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-15       [2, 1024, 8, 8]           [2, 1024, 8, 8]           10,240\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-16       [2, 1024, 8, 8]           [2, 1024, 8, 8]           1,049,600\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-31                    [2, 1024, 8, 8]           [2, 1024, 8, 8]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-32                  [2, 1024, 8, 8]           [2, 256, 8, 8]            262,400\n",
       "=================================================================================================================================================\n",
       "Total params: 7,324,288\n",
       "Trainable params: 7,324,288\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.29\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 2.10\n",
       "Forward/backward pass size (MB): 219.81\n",
       "Params size (MB): 29.30\n",
       "Estimated Total Size (MB): 251.20\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.segformer_simple import MiT\n",
    "from torchinfo import summary\n",
    "\n",
    "args = {\n",
    "    'dims': (32, 64, 160, 256),\n",
    "    'heads': (1, 2, 5, 8),\n",
    "    'ff_expansion': (8, 8, 4, 4),\n",
    "    'reduction_ratio': (8, 4, 2, 1),\n",
    "    'num_layers': (2,2,2,2),\n",
    "    'channels': 4,\n",
    "    'stage_kernel_stride_pad': [(7, 4, 3), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1)],\n",
    "}\n",
    "model = MiT(**args)\n",
    "summary(model,col_names=(\"input_size\",\"output_size\",\"num_params\",),row_settings=('var_names','depth'), depth=10,input_size=(2, 4, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                      Input Shape               Output Shape              Param #\n",
       "=================================================================================================================================================\n",
       "MiT (MiT)                                                              [2, 4, 256, 256]          [2, 256, 16, 16]          --\n",
       "├─ModuleList (stages): 1-1                                             --                        --                        --\n",
       "│    └─ModuleList (0): 2-1                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-1                                            [2, 4, 256, 256]          [2, 64, 16384]            --\n",
       "│    │    └─Conv2d (1): 3-2                                            [2, 64, 128, 128]         [2, 36, 128, 128]         2,340\n",
       "│    │    └─ModuleList (2): 3-3                                        --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-1                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-1                                 [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-1                       [2, 36, 128, 128]         [2, 36, 128, 128]         72\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-2            [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-1                     [2, 36, 128, 128]         [2, 36, 128, 128]         1,296\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-2                    [2, 36, 128, 128]         [2, 72, 16, 16]           165,888\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-3                   [2, 36, 128, 128]         [2, 36, 128, 128]         1,296\n",
       "│    │    │    │    └─PreNorm (1): 5-2                                 [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-3                       [2, 36, 128, 128]         [2, 36, 128, 128]         72\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-4                    [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-4                  [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-1                   [2, 36, 128, 128]         [2, 288, 128, 128]        10,656\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-2                 [2, 288, 128, 128]        [2, 288, 128, 128]        --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-1        [2, 288, 128, 128]        [2, 288, 128, 128]        --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-1        [2, 288, 128, 128]        [2, 288, 128, 128]        2,880\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-2        [2, 288, 128, 128]        [2, 288, 128, 128]        83,232\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-3                     [2, 288, 128, 128]        [2, 288, 128, 128]        --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-4                   [2, 288, 128, 128]        [2, 36, 128, 128]         10,404\n",
       "│    │    │    └─ModuleList (1): 4-2                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-3                                 [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-5                       [2, 36, 128, 128]         [2, 36, 128, 128]         72\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-6            [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-5                     [2, 36, 128, 128]         [2, 36, 128, 128]         1,296\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-6                    [2, 36, 128, 128]         [2, 72, 16, 16]           165,888\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-7                   [2, 36, 128, 128]         [2, 36, 128, 128]         1,296\n",
       "│    │    │    │    └─PreNorm (1): 5-4                                 [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-7                       [2, 36, 128, 128]         [2, 36, 128, 128]         72\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-8                    [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-8                  [2, 36, 128, 128]         [2, 36, 128, 128]         --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-5                   [2, 36, 128, 128]         [2, 288, 128, 128]        10,656\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-6                 [2, 288, 128, 128]        [2, 288, 128, 128]        --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-2        [2, 288, 128, 128]        [2, 288, 128, 128]        --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-3        [2, 288, 128, 128]        [2, 288, 128, 128]        2,880\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-4        [2, 288, 128, 128]        [2, 288, 128, 128]        83,232\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-7                     [2, 288, 128, 128]        [2, 288, 128, 128]        --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-8                   [2, 288, 128, 128]        [2, 36, 128, 128]         10,404\n",
       "│    └─ModuleList (1): 2-2                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-4                                            [2, 36, 128, 128]         [2, 324, 4096]            --\n",
       "│    │    └─Conv2d (1): 3-5                                            [2, 324, 64, 64]          [2, 72, 64, 64]           23,400\n",
       "│    │    └─ModuleList (2): 3-6                                        --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-3                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-5                                 [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-9                       [2, 72, 64, 64]           [2, 72, 64, 64]           144\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-10           [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-9                     [2, 72, 64, 64]           [2, 72, 64, 64]           5,184\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-10                   [2, 72, 64, 64]           [2, 144, 16, 16]          165,888\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-11                  [2, 72, 64, 64]           [2, 72, 64, 64]           5,184\n",
       "│    │    │    │    └─PreNorm (1): 5-6                                 [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-11                      [2, 72, 64, 64]           [2, 72, 64, 64]           144\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-12                   [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-12                 [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-9                   [2, 72, 64, 64]           [2, 576, 64, 64]          42,048\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-10                [2, 576, 64, 64]          [2, 576, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-3        [2, 576, 64, 64]          [2, 576, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-5        [2, 576, 64, 64]          [2, 576, 64, 64]          5,760\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-6        [2, 576, 64, 64]          [2, 576, 64, 64]          332,352\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-11                    [2, 576, 64, 64]          [2, 576, 64, 64]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-12                  [2, 576, 64, 64]          [2, 72, 64, 64]           41,544\n",
       "│    │    │    └─ModuleList (1): 4-4                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-7                                 [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-13                      [2, 72, 64, 64]           [2, 72, 64, 64]           144\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-14           [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-13                    [2, 72, 64, 64]           [2, 72, 64, 64]           5,184\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-14                   [2, 72, 64, 64]           [2, 144, 16, 16]          165,888\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-15                  [2, 72, 64, 64]           [2, 72, 64, 64]           5,184\n",
       "│    │    │    │    └─PreNorm (1): 5-8                                 [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-15                      [2, 72, 64, 64]           [2, 72, 64, 64]           144\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-16                   [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-16                 [2, 72, 64, 64]           [2, 72, 64, 64]           --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-13                  [2, 72, 64, 64]           [2, 576, 64, 64]          42,048\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-14                [2, 576, 64, 64]          [2, 576, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-4        [2, 576, 64, 64]          [2, 576, 64, 64]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-7        [2, 576, 64, 64]          [2, 576, 64, 64]          5,760\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-8        [2, 576, 64, 64]          [2, 576, 64, 64]          332,352\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-15                    [2, 576, 64, 64]          [2, 576, 64, 64]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-16                  [2, 576, 64, 64]          [2, 72, 64, 64]           41,544\n",
       "│    └─ModuleList (2): 2-3                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-7                                            [2, 72, 64, 64]           [2, 648, 1024]            --\n",
       "│    │    └─Conv2d (1): 3-8                                            [2, 648, 32, 32]          [2, 180, 32, 32]          116,820\n",
       "│    │    └─ModuleList (2): 3-9                                        --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-5                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-9                                 [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-17                      [2, 180, 32, 32]          [2, 180, 32, 32]          360\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-18           [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-17                    [2, 180, 32, 32]          [2, 180, 32, 32]          32,400\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-18                   [2, 180, 32, 32]          [2, 360, 16, 16]          259,200\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-19                  [2, 180, 32, 32]          [2, 180, 32, 32]          32,400\n",
       "│    │    │    │    └─PreNorm (1): 5-10                                [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-19                      [2, 180, 32, 32]          [2, 180, 32, 32]          360\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-20                   [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-20                 [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-17                  [2, 180, 32, 32]          [2, 720, 32, 32]          130,320\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-18                [2, 720, 32, 32]          [2, 720, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-5        [2, 720, 32, 32]          [2, 720, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-9        [2, 720, 32, 32]          [2, 720, 32, 32]          7,200\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-10       [2, 720, 32, 32]          [2, 720, 32, 32]          519,120\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-19                    [2, 720, 32, 32]          [2, 720, 32, 32]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-20                  [2, 720, 32, 32]          [2, 180, 32, 32]          129,780\n",
       "│    │    │    └─ModuleList (1): 4-6                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-11                                [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-21                      [2, 180, 32, 32]          [2, 180, 32, 32]          360\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-22           [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-21                    [2, 180, 32, 32]          [2, 180, 32, 32]          32,400\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-22                   [2, 180, 32, 32]          [2, 360, 16, 16]          259,200\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-23                  [2, 180, 32, 32]          [2, 180, 32, 32]          32,400\n",
       "│    │    │    │    └─PreNorm (1): 5-12                                [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-23                      [2, 180, 32, 32]          [2, 180, 32, 32]          360\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-24                   [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-24                 [2, 180, 32, 32]          [2, 180, 32, 32]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-21                  [2, 180, 32, 32]          [2, 720, 32, 32]          130,320\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-22                [2, 720, 32, 32]          [2, 720, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-6        [2, 720, 32, 32]          [2, 720, 32, 32]          --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-11       [2, 720, 32, 32]          [2, 720, 32, 32]          7,200\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-12       [2, 720, 32, 32]          [2, 720, 32, 32]          519,120\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-23                    [2, 720, 32, 32]          [2, 720, 32, 32]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-24                  [2, 720, 32, 32]          [2, 180, 32, 32]          129,780\n",
       "│    └─ModuleList (3): 2-4                                             --                        --                        --\n",
       "│    │    └─Unfold (0): 3-10                                           [2, 180, 32, 32]          [2, 1620, 256]            --\n",
       "│    │    └─Conv2d (1): 3-11                                           [2, 1620, 16, 16]         [2, 256, 16, 16]          414,976\n",
       "│    │    └─ModuleList (2): 3-12                                       --                        --                        --\n",
       "│    │    │    └─ModuleList (0): 4-7                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-13                                [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-25                      [2, 256, 16, 16]          [2, 256, 16, 16]          512\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-26           [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-25                    [2, 256, 16, 16]          [2, 256, 16, 16]          65,536\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-26                   [2, 256, 16, 16]          [2, 512, 16, 16]          131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-27                  [2, 256, 16, 16]          [2, 256, 16, 16]          65,536\n",
       "│    │    │    │    └─PreNorm (1): 5-14                                [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-27                      [2, 256, 16, 16]          [2, 256, 16, 16]          512\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-28                   [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-28                 [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-25                  [2, 256, 16, 16]          [2, 1024, 16, 16]         263,168\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-26                [2, 1024, 16, 16]         [2, 1024, 16, 16]         --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-7        [2, 1024, 16, 16]         [2, 1024, 16, 16]         --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-13       [2, 1024, 16, 16]         [2, 1024, 16, 16]         10,240\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-14       [2, 1024, 16, 16]         [2, 1024, 16, 16]         1,049,600\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-27                    [2, 1024, 16, 16]         [2, 1024, 16, 16]         --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-28                  [2, 1024, 16, 16]         [2, 256, 16, 16]          262,400\n",
       "│    │    │    └─ModuleList (1): 4-8                                   --                        --                        --\n",
       "│    │    │    │    └─PreNorm (0): 5-15                                [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-29                      [2, 256, 16, 16]          [2, 256, 16, 16]          512\n",
       "│    │    │    │    │    └─EfficientSelfAttention (fn): 6-30           [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Conv2d (to_q): 7-29                    [2, 256, 16, 16]          [2, 256, 16, 16]          65,536\n",
       "│    │    │    │    │    │    └─Conv2d (to_kv): 7-30                   [2, 256, 16, 16]          [2, 512, 16, 16]          131,072\n",
       "│    │    │    │    │    │    └─Conv2d (to_out): 7-31                  [2, 256, 16, 16]          [2, 256, 16, 16]          65,536\n",
       "│    │    │    │    └─PreNorm (1): 5-16                                [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    └─LayerNorm (norm): 6-31                      [2, 256, 16, 16]          [2, 256, 16, 16]          512\n",
       "│    │    │    │    │    └─MixFeedForward (fn): 6-32                   [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    │    └─Sequential (net): 7-32                 [2, 256, 16, 16]          [2, 256, 16, 16]          --\n",
       "│    │    │    │    │    │    │    └─Conv2d (0): 8-29                  [2, 256, 16, 16]          [2, 1024, 16, 16]         263,168\n",
       "│    │    │    │    │    │    │    └─DsConv2d (1): 8-30                [2, 1024, 16, 16]         [2, 1024, 16, 16]         --\n",
       "│    │    │    │    │    │    │    │    └─Sequential (net): 9-8        [2, 1024, 16, 16]         [2, 1024, 16, 16]         --\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (0): 10-15       [2, 1024, 16, 16]         [2, 1024, 16, 16]         10,240\n",
       "│    │    │    │    │    │    │    │    │    └─Conv2d (1): 10-16       [2, 1024, 16, 16]         [2, 1024, 16, 16]         1,049,600\n",
       "│    │    │    │    │    │    │    └─GELU (2): 8-31                    [2, 1024, 16, 16]         [2, 1024, 16, 16]         --\n",
       "│    │    │    │    │    │    │    └─Conv2d (3): 8-32                  [2, 1024, 16, 16]         [2, 256, 16, 16]          262,400\n",
       "=================================================================================================================================================\n",
       "Total params: 8,225,056\n",
       "Trainable params: 8,225,056\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 20.98\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 2.10\n",
       "Forward/backward pass size (MB): 984.02\n",
       "Params size (MB): 32.90\n",
       "Estimated Total Size (MB): 1019.02\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.segformer_simple import MiT\n",
    "from torchinfo import summary\n",
    "\n",
    "args = {\n",
    "    'dims': (36, 72, 180, 256),\n",
    "    'heads': (1, 2, 5, 8),\n",
    "    'ff_expansion': (8, 8, 4, 4),\n",
    "    'reduction_ratio': (8, 4, 2, 1),\n",
    "    'num_layers': (2,2,2,2),\n",
    "    'channels': 4,\n",
    "    'stage_kernel_stride_pad': [(4, 2, 1), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1)],\n",
    "}\n",
    "model = MiT(**args)\n",
    "summary(model,col_names=(\"input_size\",\"output_size\",\"num_params\",),row_settings=('var_names','depth'), depth=10,input_size=(2, 4, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Segformerwithcarbon.__init__() got an unexpected keyword argument 'stage_kernel_stride_pad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheads\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSegformerwithcarbon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m summary(model,col_names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_params\u001b[39m\u001b[38;5;124m\"\u001b[39m,),row_settings\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar_names\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m), depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Segformerwithcarbon.__init__() got an unexpected keyword argument 'stage_kernel_stride_pad'"
     ]
    }
   ],
   "source": [
    "from models.segformer_simple import Segformerwithcarbon\n",
    "from torchinfo import summary\n",
    "args = {\n",
    "    'dims': (32, 64, 160, 256),\n",
    "    'heads': (1, 2, 5, 8),\n",
    "    'ff_expansion': (8, 8, 4, 4),\n",
    "    'reduction_ratio': (8, 4, 2, 1),\n",
    "    'num_layers': (2,2,2,2),\n",
    "    'channels': 4,\n",
    "    'stage_kernel_stride_pad': [(7, 2, 3), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1)],\n",
    "    'decoder_dim': 512,\n",
    "    'num_classes': 7\n",
    "}\n",
    "model = Segformerwithcarbon(**args)\n",
    "summary(model,col_names=(\"input_size\",\"output_size\",\"num_params\",),row_settings=('var_names','depth'), depth=10,input_size=(2, 4, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Workspace\\CarbonCapturePredict\\wandb\\run-20240414_031059-y3jnkw6s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/CCP/runs/y3jnkw6s' target=\"_blank\">tough-feather-95</a></strong> to <a href='https://wandb.ai/waooang/CCP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/CCP' target=\"_blank\">https://wandb.ai/waooang/CCP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/CCP/runs/y3jnkw6s' target=\"_blank\">https://wandb.ai/waooang/CCP/runs/y3jnkw6s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'notes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m\n\u001b[0;32m     11\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m     12\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[0;32m     14\u001b[0m project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCCP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegformerwithcarbon_B1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotes\u001b[49m)\n\u001b[0;32m     29\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'notes'"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "# lr = 0.0001\n",
    "# batch_size = 2\n",
    "# epochs = 10\n",
    "# fp = 16\n",
    "# model_name = \"Segformer\"\n",
    "# cls_lambda = 1\n",
    "# reg_lambda = 1\n",
    "# checkpoint_path = \"checkpoint\"\n",
    "# notes=\"Segformerwithcarbon_B1\"\n",
    "# wandb.login()\n",
    "# wandb.init(\n",
    "# # set the wandb project where this run will be logged\n",
    "# project=\"CCP\",\n",
    "# # track hyperparameters and run metadata\n",
    "# config={\n",
    "# \"learning_rate\": lr,\n",
    "# \"batch_size\": batch_size,\n",
    "# \"epochs\": epochs,\n",
    "# \"fp\": fp,\n",
    "# \"model_name\": model_name,\n",
    "# \"cls_lambda\": cls_lambda,\n",
    "# \"reg_lambda\": reg_lambda,\n",
    "# \"checkpoint_path\": checkpoint_path\n",
    "# },\n",
    "# notes =notes\n",
    "# )\n",
    "# print(wandb.notes)\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
